
services:
  backend:
    build:
      context: .
      dockerfile: python_back_end/Dockerfile
    container_name: backend
    ports:
      - "8000:8000"
    volumes:
      - ./python_back_end:/app
    # ---- GPU magic --------------------------------------------------
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_FLASH_ATTENTION=1
    networks:
      - ollama-n8n-network
    restart: unless-stopped

 # frontend:
   # image: nginx:alpine
    #container_name: frontend
    #volumes:
     # - ./front_end:/usr/share/nginx/html
    #etworks:
    #  - ollama-n8n-network
    #restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    # ---- GPU magic --------------------------------------------------
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_FLASH_ATTENTION=1
    networks:
      - ollama-n8n-network
    tty: true
    restart: unless-stopped

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "3000:8080"
    depends_on:
      - ollama
    networks:
      - ollama-n8n-network
    restart: unless-stopped

  n8n:
    image: n8nio/n8n
    container_name: n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=${N8N_HOST:-localhost}
      - N8N_PORT=5678
      - WEBHOOK_URL=http://${N8N_HOST:-localhost}:5678/
      - N8N_PROTOCOL=http
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      - ollama
    networks:
      - ollama-n8n-network
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    ports:
      - "9000:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./front_end:/usr/share/nginx/html
    depends_on:
      #- frontend
      - backend
      - webui
      - ollama
      - n8n
      - agent-zero-run
    networks:
      - ollama-n8n-network
    restart: unless-stopped

  agent-zero-run:
    image: frdel/agent-zero-run
    container_name: agent-zero-run
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "5000:5000"
    depends_on:
      - ollama
    networks:
      - ollama-n8n-network
    restart: unless-stopped

volumes:
  ollama:
  n8n_data:

networks:
  ollama-n8n-network:
    name: ollama-n8n-network
    driver: bridge
