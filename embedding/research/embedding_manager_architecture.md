# EmbeddingManager Module Architecture & Design

## Overview
The `EmbeddingManager` class is a comprehensive orchestrator for n8n workflow embedding generation and vector database operations. It serves as the primary interface between n8n workflow processing and PostgreSQL-based vector storage using pgvector.

## Core Architecture

### Dependencies & Integration Stack
```python
# Core LangChain Components
from langchain_core.documents import Document      # Document abstraction
from langchain_postgres import PGVector           # PostgreSQL vector store
from langchain_community.embeddings import HuggingFaceEmbeddings  # Local embeddings

# Database Layer
import psycopg                                   # PostgreSQL driver (v3)
from psycopg.rows import dict_row               # Dictionary row factory

# Internal Modules
from .config import EmbeddingConfig             # Configuration management
from .workflow_processor import WorkflowProcessor  # n8n workflow parsing
```

### Class Architecture

#### Core Components
1. **Configuration Layer** (`EmbeddingConfig`)
   - Database connection parameters
   - Embedding model configuration
   - Processing parameters (chunk size, overlap)
   - Collection management settings

2. **Embedding Engine** (`HuggingFaceEmbeddings`)
   - Local CPU-based embedding generation
   - Normalized embeddings for consistent similarity scoring
   - Model: Configurable (default likely sentence-transformers)

3. **Vector Storage** (`PGVector`)
   - PostgreSQL + pgvector extension
   - Configurable distance strategies
   - JSONB metadata support
   - Collection-based organization

4. **Workflow Processing** (`WorkflowProcessor`)
   - n8n workflow JSON parsing
   - Text chunking with overlap
   - Metadata extraction (node types, source repos)

## Key Design Patterns

### Lazy Initialization Pattern
```python
def initialize_embeddings(self):
    # Only initialize when first needed
    if not self.embeddings:
        self.embeddings = HuggingFaceEmbeddings(...)
```

### Batch Processing Pattern
```python
def _add_document_batch(self, documents: List[Document]) -> int:
    # Process documents in configurable batch sizes
    # Filters empty documents
    # Provides error isolation per batch
```

### Connection Management Pattern
```python
with psycopg.connect(**conn_params) as conn:
    with conn.cursor(row_factory=dict_row) as cur:
        # Automatic connection cleanup
        # Dictionary-style row access
        # Transaction management
```

## Method Categories

### 1. Initialization Methods
- `__init__()` - Object construction with config
- `initialize_embeddings()` - Lazy embedding model loading
- `initialize_vector_store()` - PGVector setup
- `setup_database()` - pgvector extension + table creation

### 2. Data Ingestion Methods
- `add_workflows_from_directory()` - Bulk workflow processing
- `_add_document_batch()` - Batch insertion with validation
- `process_all_workflow_directories()` - Multi-source processing

### 3. Search & Retrieval Methods
- `search_workflows()` - Semantic similarity search
- `search_workflows_with_score()` - Search with confidence scores
- `get_collection_stats()` - Analytics and metadata queries

### 4. Management Methods
- `delete_collection()` - Collection cleanup
- `test_embedding_pipeline()` - Health check and validation

## Data Flow Architecture

### Ingestion Pipeline
```
n8n Workflows (JSON) 
    ↓
WorkflowProcessor (chunking, metadata extraction)
    ↓
Document objects (text + metadata)
    ↓
HuggingFaceEmbeddings (vector generation)
    ↓
PGVector (PostgreSQL storage)
```

### Search Pipeline
```
User Query (text)
    ↓
HuggingFaceEmbeddings (query vectorization)
    ↓
PGVector (similarity search)
    ↓
Ranked Documents (with scores)
```

## Database Schema Integration

### Table Structure
```sql
-- Auto-generated by PGVector
langchain_pg_embedding_{collection_name}
    - id: Primary key
    - embedding: vector(dimensions)  -- pgvector type
    - document: text                 -- Chunked workflow content
    - cmetadata: jsonb              -- Workflow metadata
```

### Metadata Schema
```json
{
    "source_repo": "repo_name",
    "node_types": ["array", "of", "n8n", "nodes"],
    "workflow_id": "unique_id",
    "chunk_index": 0,
    "total_chunks": 3
}
```

## Performance Considerations

### Optimization Strategies
1. **Batch Processing** - Configurable batch sizes (default: 50)
2. **Connection Pooling** - Context managers for connection lifecycle
3. **Lazy Loading** - Initialize components only when needed
4. **CPU-based Embeddings** - Avoids GPU dependency issues
5. **Normalized Embeddings** - Consistent similarity calculations

### Scalability Features
- Configurable max workflow limits
- Chunked document processing
- JSONB for flexible metadata
- Distance strategy configuration

## Error Handling Strategy

### Layered Error Management
1. **Database Level** - Connection and query error handling
2. **Processing Level** - Document validation and filtering
3. **Batch Level** - Isolated batch failure recovery
4. **Application Level** - Comprehensive logging and error reporting

### Recovery Mechanisms
- Empty document filtering
- Batch-wise processing (failure isolation)
- Detailed error logging
- Graceful degradation

## Integration Points

### External Dependencies
- **PostgreSQL** - Primary storage backend
- **pgvector** - Vector similarity operations
- **HuggingFace Models** - Local embedding generation
- **n8n Workflows** - Source data format

### Internal Dependencies
- **EmbeddingConfig** - Configuration management
- **WorkflowProcessor** - Data preprocessing
- **Logging System** - Observability and debugging

## Usage Patterns

### Typical Workflow
```python
# 1. Initialize with configuration
manager = EmbeddingManager(config)

# 2. Setup database (one-time)
manager.setup_database()

# 3. Process workflows
results = manager.process_all_workflow_directories()

# 4. Search workflows
documents = manager.search_workflows("email automation", k=5)

# 5. Get analytics
stats = manager.get_collection_stats()
```

This architecture provides a robust, scalable foundation for n8n workflow semantic search and similarity matching.