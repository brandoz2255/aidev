"""Vibe Coding Core Logic Module

This module contains the core vibe agent functionality extracted from main.py
to keep the vibe coding logic organized and maintainable.
"""

import os
import sys
import logging
import requests
from typing import List, Dict, Any, Tuple
from model_manager import unload_all_models, reload_models_if_needed

logger = logging.getLogger(__name__)

# Add the ollama_cli directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ollama_cli'))
from vibe_agent import VibeAgent

# Initialize vibe agent
vibe_agent = None

def initialize_vibe_agent(project_dir: str = None) -> None:
    """Initialize the vibe agent with the given project directory."""
    global vibe_agent
    try:
        vibe_agent = VibeAgent(project_dir=project_dir or os.getcwd())
        logger.info("âœ… VibeAgent initialized successfully")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize VibeAgent: {e}")
        vibe_agent = None

def get_vibe_agent() -> VibeAgent:
    """Get the initialized vibe agent instance."""
    if vibe_agent is None:
        initialize_vibe_agent()
    return vibe_agent

async def process_vibe_command_with_context(
    message: str, 
    files: List[Dict], 
    terminal_history: List[str], 
    model: str,
    ollama_url: str,
    api_key: str = "key"
) -> Tuple[str, List[Dict]]:
    """
    Process vibe coding command with full context using the vibe agent logic
    """
    # Create context from files and terminal history
    context = ""
    if files:
        context += "CURRENT FILES:\n"
        for file_info in files[:3]:  # Limit to 3 files to avoid context overflow
            context += f"- {file_info.get('name', 'unnamed')}: {file_info.get('content', '')[:200]}...\n"
        context += "\n"
    
    if terminal_history:
        context += "TERMINAL HISTORY:\n"
        context += "\n".join(terminal_history[-5:])  # Last 5 lines
        context += "\n\n"
    
    # Generate vibe agent response using Ollama
    system_prompt = """You are a Vibe Coding AI assistant. You help users build projects through natural conversation and voice commands.

GUIDELINES:
- Generate practical, executable coding steps
- Be conversational and encouraging  
- Focus on what the user wants to build
- Provide specific file names and code snippets
- Keep responses under 200 words for voice synthesis"""
    
    user_prompt = f"{context}USER REQUEST: {message}"
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
    }
    
    logger.info(f"â†’ Asking Ollama with model {model} for vibe coding")
    headers = {"Authorization": f"Bearer {api_key}"} if api_key != "key" else {}
    resp = requests.post(f"{ollama_url}/api/chat", json=payload, headers=headers, timeout=120)
    resp.raise_for_status()
    
    vibe_response = resp.json().get("message", {}).get("content", "").strip()
    
    # Generate coding steps based on the message
    steps = generate_vibe_steps(message, vibe_response)
    
    return vibe_response, steps

def generate_vibe_steps(message: str, response: str) -> List[Dict]:
    """
    Generate coding steps based on the user message and AI response
    """
    steps = []
    message_lower = message.lower()
    
    # Python file creation
    if "create" in message_lower and ("file" in message_lower or ".py" in message_lower):
        filename = "script.py"
        if ".py" in message:
            import re
            match = re.search(r'(\w+\.py)', message)
            if match:
                filename = match.group(1)
        
        steps.append({
            "id": "1",
            "description": f"Creating file: {filename}",
            "action": "create_file",
            "target": filename,
            "content": f"# Generated by Vibe Coding AI\n# {message}\n\nprint('Hello from {filename}!')",
            "completed": False
        })
    
    # Web app/API creation
    elif "api" in message_lower or "web" in message_lower or "flask" in message_lower or "fastapi" in message_lower:
        steps.extend([
            {
                "id": "1",
                "description": "Creating main API file",
                "action": "create_file", 
                "target": "app.py",
                "content": "# FastAPI/Flask Web Application\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get('/')\ndef hello():\n    return {'message': 'Hello from Vibe Coding!'}",
                "completed": False
            },
            {
                "id": "2", 
                "description": "Installing dependencies",
                "action": "run_command",
                "target": "pip install fastapi uvicorn",
                "completed": False
            },
            {
                "id": "3",
                "description": "Running the application", 
                "action": "run_command",
                "target": "uvicorn app:app --reload",
                "completed": False
            }
        ])
    
    # If no specific steps, create a generic one
    if not steps:
        steps.append({
            "id": "1", 
            "description": "Creating script based on your request",
            "action": "create_file",
            "target": "vibe_script.py",
            "content": f"# Script for: {message}\n# Generated by Vibe Coding AI\n\nprint('Starting your vibe coding project!')\n# Add your code here",
            "completed": False
        })
    
    return steps

async def execute_vibe_coding_with_model_management(
    message: str,
    files: List[Dict[str, Any]] = None,
    terminal_history: List[str] = None,
    model: str = "mistral",
    ollama_url: str = "http://ollama:11434",
    api_key: str = "key"
) -> Tuple[str, List[Dict]]:
    """
    Execute vibe coding with intelligent model management.
    Unloads models â†’ Executes vibe agent â†’ Reloads models.
    """
    files = files or []
    terminal_history = terminal_history or []
    
    try:
        # Phase 1: Unload models to free GPU memory for vibe agent processing
        logger.info("ðŸ¤– Phase 1: Starting vibe coding - clearing GPU memory for vibe agent")
        unload_all_models()
        
        # Phase 2: Execute vibe agent processing
        logger.info("âš¡ Phase 2: Executing vibe agent with model")
        vibe_response, steps = await process_vibe_command_with_context(
            message, files, terminal_history, model, ollama_url, api_key
        )
        
        # Phase 3: Reload models for potential TTS
        logger.info("ðŸ”„ Phase 3: Reloading models after vibe processing")
        reload_models_if_needed()
        
        return vibe_response, steps
        
    except Exception as e:
        logger.error(f"Vibe coding processing failed: {e}")
        # Ensure models are reloaded even on error
        reload_models_if_needed()
        raise